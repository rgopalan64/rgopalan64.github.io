---
title: "PROJECT R MARKDOWN"
output: html_document
---

This is an R Markdown document for the PRACTICAL MACHINE LEARNING PROJECT
'directory' is a character vector of length 1 indicating the location of the CSV files (the directory where the action takes place)

```{r}
pmlproject3 <- function(directory) {
  
  
  setwd(directory)
  
  filerec <- read.csv("pml-training.csv",header=TRUE,na.strings="NA |  ", stringsAsFactors=FALSE)
  # print(head(filerec,5))
  
  # COPY FILEREC TO FILEREC2
  filerec2 <- data.frame(filerec)
  NR <- nrow(filerec2)
  Nlimit <- (NR/10)
  NC <- ncol(filerec2)
  print("NROWS IN RAW DATA SET = ")
  print(NR)
  print("NCOLS IN RAW DATA SET = ")
  print(NC)
  print("Nlimit")
  print(Nlimit)
  
  
  # First replace blanks or NaNs in the data set with NA values
  
  filerec2[filerec2 == ""] <- NA
  filerec2[filerec2 == NaN] <- NA
  
  # NOW REMOVE COLS WITH ALL MISSING VALUES FROM FILEREC2
  filerec2 <- filerec2[,colSums(is.na(filerec2)) == 0]
  
  # RECALCULATE COLS TO SEE HOW MANY ELIMINATED
  NC2 <- ncol(filerec2)
  print("NEW COLS AFTER ELIMINATION OF BLANK COLS =")
  print(NC2)
  print("NEW ROWS")
  print(nrow(filerec2))
  
  # print(summary(filerec2))
  
  # CREATE NUMERIC DUMMY VARIABLES 1,2,3,4,5 to REPRESENT CLASSES A, B, C, D, E
  filerec2$newclasse[filerec2$classe == "A"] <- 1
  filerec2$newclasse[filerec2$classe == "B"] <- 2
  filerec2$newclasse[filerec2$classe == "C"] <- 3
  filerec2$newclasse[filerec2$classe == "D"] <- 4
  filerec2$newclasse[filerec2$classe == "E"] <- 5
  
  # FIND NUMERIC COLS TO SERVE AS FEATURES (not strictly necessary, but this was ONE version tried)
  nums <- sapply(filerec2, is.numeric)
  
  # CREATE FILEREC3 containing ONLY the NUMERIC COLS in filerec2
  filerec3 <- filerec2[nums]
  print(ncol(filerec3))
  
  set.seed(1)
  # CREATE a TRAINING DATA SET
  inTrain <- createDataPartition(y = filerec3$newclasse, p = 0.7, list = FALSE)
  training <- filerec3[inTrain, ]
  print("NROW IN TRAINING DATA SET")
  print(nrow(training))

  testing <- filerec3[-inTrain, ]
  
  
  # CREATE A SEPARATE COL yy WHICH REPRESENTS A NUMERIC REPRESENTATION OF FIVE CLASSES
  yy <- training$newclasse
  # sapply(yy, as.numeric)
  
  NC3 <- ncol(filerec3)
  
  for(j in 1: NC3){
    if(training[, j] != training$newclasse){
      xx <- training[, j]
      sapply(xx, as.numeric)
      # print(xx)
      # FIND CORRELATION BETWEEN COL xx and DEPENDENT VAR yy
      zz <- round(cor(xx,yy), digits = 6)
      print(colnames(training)[j])
      # print(training$classe)
      # PRINT THE CORRELATION
      print(zz)
    }
  }
  

  qplot(accel_arm_x, magnet_arm_x, color = newclasse, data = testing)
  qplot(magnet_belt_y, newclasse, color = newclasse, data = testing)
  
  # NOW ADD BACK THE CLASSE FACTOR VARIABLE
  # CHANGE CLASSE AND NEWCLASSE TO FACTORS TO RUN TREE CLASSIFICATION METHODS
  filerec3$classe <- filerec2[,"classe"]
  filerec3$classe <- factor(filerec3$classe)
  filerec3$newclasse <- factor(filerec3$newclasse)
  print(levels(filerec3$newclasse))
  
  # create training and test sets once again after adding classe 
  set.seed(1)
  inTrain2 <- createDataPartition(y = filerec3$classe, p = 0.7, list = FALSE)
  training2 <- filerec3[inTrain2, ]
  
  print("NROWS in TRAINING2")
  print(nrow(training2))
  testing2 <- filerec3[-inTrain2, ]
  
  # FIT A DECISION TREE WITH VARIABLES HAVING HIGH CORRELATION WITH PREDICTOR
  # modFit = train(classe ~ pitch_forearm+pitch_arm+magnet_belt_y+magnet_belt_z+accel_arm_x+ accel_forearm_x+ magnet_arm_x+ magnet_arm_y+ magnet_dumbbell_z+ magnet_forearm_x, method = "rpart", data=training2)
  
  # SIMPLER TREE WITH 4 PREDICTORS
  # modFit = train(classe ~ pitch_forearm+ magnet_belt_y+ magnet_belt_z+ magnet_dumbbell_z, method = "rpart", data=training2)
  # print(modFit$finalModel)
  # predtree <- predict(modFit, newdata=testing2)
  # testing2$predRight <- predtree == testing2$classe
  # table(predtree, testing2$classe)
  # print(summary(testing2$predRight))
  
  
  
  # NOW FIT A RANDOM FOREST MODEL - USE randomForest directly, not CARET to do this...
  modFit.rf = randomForest(classe ~ pitch_forearm+pitch_arm+magnet_belt_y+magnet_belt_z+accel_arm_x+ accel_forearm_x+ magnet_arm_x+ magnet_arm_y+ magnet_dumbbell_z+ magnet_forearm_x, data=training2, mtry = 6, importance = TRUE)
  # NOW BUILD SIMPLER RANDOM FOREST WITH JUST 4 VARIABLES
  # modFit.rf = randomForest(classe ~ pitch_forearm+magnet_belt_y+ magnet_belt_z+ magnet_dumbbell_z, data=training2, mtry = 6, importance = TRUE)
  importance(modFit.rf)
  
  # NEXT TWO LINES PERTAIN TO BUILDING A RANDOM FOREST WITH THE CARET PACKAGE
  # modFit.rf <- train(classe ~ pitch_forearm+pitch_arm+magnet_belt_y+magnet_belt_z+accel_arm_x+ accel_forearm_x+ magnet_arm_x+ magnet_arm_y+ magnet_dumbbell_z+ magnet_forearm_x, data=training2 , method = "rf", PROX=TRUE)
  # modFit
  # modFit$finalModel
  
  
  # NOW TEST THE FITTED MODEL USING TESTING2 (OUT OF SAMPLE ERROR FOUND HERE)
  yhat.rf = predict(modFit.rf, type="response", newdata = testing2)
  testing2$predRight <- (yhat.rf == testing2$classe)
  summary(testing2$predRight)
  # PRINT a TABLE OF CROSS VALIDATION ACCURACY
  table(yhat.rf, testing2$newclasse)
  
  
  # NOW MAKE PREDICTIONS FOR NEW DATA SET
  filetest <- read.csv("pml-testing.csv",header=TRUE,na.strings="NA |  ", stringsAsFactors=FALSE)
  filetest2 <- data.frame(filetest)
  
  filetest2[filetest2 == ""] <- NA
  filetest2[filetest2 == NaN] <- NA
  
  numstest <- sapply(filetest2, is.numeric)
  
  # FILETEST3 contains ONLY the NUMERIC COLS in filetest2
  filetest3 <- filetest2[numstest]
  
  # CODE BELOW FILLS IN MISSING VALUES IN TEST DATA SET COLS WITH COL MEDIAN IF NEEDED
  #NCT <- ncol(filetest3)
  #for(j in 1: NCT){
  #  colvect <- filetest3[,j]
  #  impute.med(colvect)
  #}
  
  # PREDICT WITH A DECISION TREE
  #predtree2 <- predict(modFit.rf, newdata=filetest3)
  #print(predtree2)
  
  # PREDICT WITH RANDOM FOREST BELOW (MOST SUCCESSFUL)
  predtree2 <- predict(modFit.rf, newdata=filetest3)
  print(predtree2)
}
```


